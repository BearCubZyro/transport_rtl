# Mid-Term Hackathon 3  Data Pipeline Design & Implementation (ETL)

## 1. Folder Structure

- **raw/**
  - `public_transport.csv`  sample public transport usage data (routes, ridership, timestamps).
  - `traffic_sensors.json`  sample traffic sensor data (avg speed, congestion index, timestamps).
- **processed/**
  - `cleaned_unified_data.csv`  generated by the ETL pipeline.
- **scripts/**
  - `etl_pipeline.py`  main ETL script (Extract, Transform, Load).
  - `data_checks.sh`  examples of `grep`/`awk`/`sed`-based pre-ingestion checks.
- **reports/**
  - `summary_by_route.csv`  generated report (avg ridership & congestion index per route).

## 2. Tools & Technologies

- **Language**: Python 3
- **Libraries**:
  - `pandas` for data manipulation and cleaning.
  - `SQLAlchemy` for database connectivity and loading.
  - `requests` for optional API-based JSON extraction.
- **Relational Database**:
  - Default: **SQLite** local file DB (`data_pipeline.db`).
  - Can be switched to **MySQL/PostgreSQL** by changing the `TRANSPORT_DB_URL` environment variable.
- **Shell utilities** (for pre-ingestion checks): `grep`, `awk`, `sed` (see `scripts/data_checks.sh`).

## 3. Pipeline Design

### 3.1 Extract

- **Public Transport (CSV)**
  - Input: `raw/transport.csv`.
  - Columns: `route_id`, `city`, `bus_stop`, `ridership`, `timing`.
- **Traffic Sensors (JSON/API)**
  - Default: local JSON at `raw/traffic.json`.
  - Fields: `route_id`, `city`, `sensor_id`, `avg_speed`, `congestion_index`, `timestamp`.
  - If environment variable `TRAFFIC_API_URL` is set, data is fetched from the API instead of the local JSON file.

### 3.2 Validation & Cleaning (Transform  Stage 1)

Performed in `etl_pipeline.py`:

- **Schema validation**:
  - Ensures required columns are present in both CSV and JSON datasets.
- **Missing values**:
  - Drops rows with missing timestamps.
  - Drops rows with non-numeric values in `ridership`, `avg_speed`, `congestion_index`.
- **Timestamp checks**:
  - Converts `timestamp` columns to proper datetime objects.
  - Drops rows where timestamps cannot be parsed.
- **Type coercion**:
  - Casts numeric fields to floats/integers where appropriate.

### 3.3 Transformation (Unification)

- Timestamps in both datasets are rounded to the **nearest hour** to align readings.
- Datasets are merged on:
  - `route_id`
  - hourly bucket (`timestamp_hour`), renamed to `event_hour`.
- Output: unified table containing, per route & hour:
  - Public transport info (city, bus_stop, ridership).
  - Traffic info (average speed, congestion index).

The unified cleaned dataset is written to:

- `processed/cleaned_unified_data.csv`

### 3.4 Load (Database)

- Uses `SQLAlchemy` to connect to a relational DB.
- Default connection string (SQLite):

  ```bash
  TRANSPORT_DB_URL="sqlite:///data_pipeline.db"
  ```

- To use MySQL/PostgreSQL, set for example:

  ```bash
  # MySQL example
  export TRANSPORT_DB_URL="mysql+pymysql://user:password@localhost:3306/transport_db"

  # PostgreSQL example
  export TRANSPORT_DB_URL="postgresql+psycopg2://user:password@localhost:5432/transport_db"
  ```

- Unified table is written as `transport_traffic` (replacing existing table each run).

### 3.5 Reporting

- A summary report is created by grouping the unified table by `route_id`, `city_public`, and `bus_stop` and computing:
  - Average `ridership` per route and stop.
  - Average `congestion_index` per route and stop.

- Output file:
  - `reports/summary_by_route.csv`

## 4. Running the Pipeline

### 4.1 Install dependencies

From the project root (`Hackathon_3_MidTerm`):

```bash
pip install -r requirements.txt
```

### 4.2 Run ETL pipeline

From the project root:

```bash
python scripts/etl_pipeline.py
```

This will:

- Read data from `raw/`.
- Validate and clean the datasets.
- Build a unified table.
- Save cleaned data to `processed/cleaned_unified_data.csv`.
- Load unified data into the relational DB (SQLite by default).
- Generate `reports/summary_by_route.csv`.

## 5. Scheduling (Cron / Airflow)

### 5.1 Example Cron Entry (Linux/Mac)

Run the pipeline every hour:

```cron
0 * * * * /usr/bin/python3 /path/to/Hackathon_3_MidTerm/scripts/etl_pipeline.py >> /var/log/transport_etl.log 2>&1
```

### 5.2 Windows Task Scheduler (Concept)

- Create a basic task that triggers **daily/hourly**.
- Action: start a program
  - Program/script: `python`
  - Arguments: `C:\\path\\to\\Hackathon_3_MidTerm\\scripts\\etl_pipeline.py`

### 5.3 Airflow (If Available)

- Create a DAG that:
  - Has a single PythonOperator calling `run_pipeline()` from `scripts/etl_pipeline.py`.
  - Is scheduled at the desired frequency (`schedule_interval="@hourly"`, for example).

## 6. Pre-Ingestion Data Checks (awk/sed/grep)

See `scripts/data_checks.sh` for examples of:

- Viewing sample rows with `head`.
- Detecting missing `ridership` values using `awk`.
- Rough timestamp format checks using `grep` and regex.
- Cleaning extra spaces with `sed`.

These quick checks help catch data quality issues before running the full ETL.
# transport_rtl
